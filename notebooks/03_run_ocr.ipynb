{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# OCR Processing for PDF Page Images\n",
    "\n",
    "This notebook applies OCR to PNG page images (from PDFs in `../tmp/input_pdf`)  \n",
    "\n",
    "and saves the extracted text and metadata as JSON files in `../tmp/output/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Currently `results = ocr_engine.run_ocr(image)` in `process_document_folder(image_dir, output_dir)` is just a dummy\n",
    "# Ideas for models/approaches to try:\n",
    "# - SuryaOCR\n",
    "# - TrOCR\n",
    "# - Huggingface: SmolVLM, ColPali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequirements\n",
    "**Ollama 0.6+ is required**\n",
    "\n",
    "```bash\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "```\n",
    "Grab the multimodal Gemma 3 build you actually have VRAM for\n",
    "\n",
    "```bash\n",
    "ollama pull gemma3:12b         # ~8 GB VRAM, good quality\n",
    "```\n",
    "\n",
    "or   \n",
    "\n",
    "```bash\n",
    "ollama pull gemma3:4b   # ~3 GB VRAM, slower but lighter\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick-start cheat sheet for running **Ollama** with Gemma locally\n",
    "\n",
    "| Step| Command    | What it does      |           \n",
    "| --------------------- | ---------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- | \n",
    "| **Install**           | curl -fsSL [https://ollama.com/install.sh](https://ollama.com/install.sh)                      |                                  Installs the daemon (`ollama serve`) and CLI (`ollama`).         |  \n",
    "| **Start the daemon**  | `ollama serve`          | Runs a local REST API on **`http://localhost:11434`** (default port). Keep this terminal open—or run it as a systemd service. |                                                          |\n",
    "| **Pull a model**      | `ollama pull gemma3:12b`<br>*(or `gemma3:4b` if VRAM-starved)*                                             | Downloads and quantizes the model. Shows up in `ollama list`.                                                                 |                                                          |\n",
    "| **Quick sanity test** | `ollama run gemma3:12b`                                                                                    | Opens an interactive REPL; type anything and get a response. Press **Ctrl-C** to exit.                                        |                                                          |\n",
    "| **Simple REST call**  | `bash curl -s http://localhost:11434/api/generate \\ -d '{\"model\":\"gemma3:12b\",\"prompt\":\"hello\"}' `         | Should stream back JSON chunks with text content.                                                                             |                                                          |\n",
    "| **Python**            | `python import ollama resp = ollama.generate(model=\"gemma3:12b\", prompt=\"hello\") print(resp[\"response\"]) ` | Works because `ollama serve` is already listening at 11434.                                                                   |                                                          |\n",
    "| **Custom host/port**  | `export OLLAMA_HOST=http://my-server:11434`                                                                | Both the CLI and the Python lib will point to this URL.                                                                       |                                                          |\n",
    "\n",
    "#### Typical friction points & fixes\n",
    "\n",
    "* **Daemon not running** → Every CLI/API request will hang. Make sure `ollama serve` (or the systemd service) is active.\n",
    "\n",
    "  ```bash\n",
    "  systemctl --user enable --now ollama  # on Linux desktop\n",
    "  ```\n",
    "\n",
    "* **GPU out of memory** → Use the 4-b model (`gemma3:4b`) or quantize further (`:int4`). `nvidia-smi` will tell you.\n",
    "\n",
    "* **Port already in use** → `OLLAMA_HOST=http://localhost:11500 ollama serve -p 11500`.\n",
    "\n",
    "* **Proxy / WSL networking issues** → Set `OLLAMA_HOST` to the real IP/port reachable from the client.\n",
    "\n",
    "---\n",
    "\n",
    "#### Recap for the notebook\n",
    "\n",
    "1. **Start Ollama once**:\n",
    "\n",
    "   ```bash\n",
    "   ollama serve\n",
    "   ```\n",
    "\n",
    "2. **Pull the model once**:\n",
    "\n",
    "   ```bash\n",
    "   ollama pull gemma3:12b\n",
    "   ```\n",
    "\n",
    "3. **Run the model from CLI**:\n",
    "\n",
    "   ```bash\n",
    "   ollama run gemma3:12b\n",
    "   ```\n",
    "\n",
    "4. **Notebook cells** use the Python client, which will silently hit `http://localhost:11434`.\n",
    "\n",
    "Nothing fancier is required. If `ollama run gemma3:12b` prints a reply, your daemon is good to go.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0. Bootstrap ----------------------------------------------------------\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv   # pip install python-dotenv\n",
    "import json, itertools\n",
    "\n",
    "nb_dir       = Path.cwd().resolve()       # .../repo/notebooks\n",
    "project_root = nb_dir.parent              # .../repo\n",
    "src_dir      = project_root / \"src\"\n",
    "\n",
    "# Make BOTH dirs importable: root → `config.*`, src → `src.*`\n",
    "sys.path.extend([str(project_root), str(src_dir)])\n",
    "\n",
    "# Load env so MODEL_TYPE=gemma is visible to pydantic\n",
    "load_dotenv(project_root / \".env\")\n",
    "\n",
    "from config.settings import settings\n",
    "assert settings.model_type == \"gemma\", settings.model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tests.local_test:OCR processing: sample\n",
      "INFO:tests.local_test:file=page_00.png (../tmp/data/sample/page_00.png)\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:tests.local_test:Written: ../tmp/output/sample/page_00.json\n",
      "INFO:tests.local_test:file=page_01.png (../tmp/data/sample/page_01.png)\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:tests.local_test:Written: ../tmp/output/sample/page_01.json\n",
      "INFO:tests.local_test:file=page_02.png (../tmp/data/sample/page_02.png)\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:tests.local_test:Written: ../tmp/output/sample/page_02.json\n",
      "INFO:tests.local_test:file=page_03.png (../tmp/data/sample/page_03.png)\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:tests.local_test:Written: ../tmp/output/sample/page_03.json\n",
      "INFO:tests.local_test:file=page_04.png (../tmp/data/sample/page_04.png)\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:tests.local_test:Written: ../tmp/output/sample/page_04.json\n",
      "INFO:tests.local_test:file=page_05.png (../tmp/data/sample/page_05.png)\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:tests.local_test:Written: ../tmp/output/sample/page_05.json\n",
      "INFO:tests.local_test:file=page_06.png (../tmp/data/sample/page_06.png)\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:tests.local_test:Written: ../tmp/output/sample/page_06.json\n",
      "INFO:tests.local_test:file=page_07.png (../tmp/data/sample/page_07.png)\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "INFO:tests.local_test:Written: ../tmp/output/sample/page_07.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR finished. JSONs live in ../tmp/output\n"
     ]
    }
   ],
   "source": [
    "from tests.local_test import process_document_folder\n",
    "from pathlib import Path\n",
    "\n",
    "INPUT_PDF_DIR     = Path(\"../tmp/input_pdf\")\n",
    "IMAGE_OUTPUT_BASE = Path(\"../tmp/data\")\n",
    "OUTPUT_ROOT       = Path(\"../tmp/output\")\n",
    "\n",
    "for pdf in INPUT_PDF_DIR.glob(\"*.pdf\"):\n",
    "    stem        = pdf.stem.lower().replace(\" \", \"_\")\n",
    "    img_dir     = IMAGE_OUTPUT_BASE / stem\n",
    "    out_dir     = OUTPUT_ROOT / stem\n",
    "\n",
    "    process_document_folder(img_dir, out_dir)\n",
    "\n",
    "print(\"OCR finished. JSONs live in\", OUTPUT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World\n"
     ]
    }
   ],
   "source": [
    "some_file = next(Path(OUTPUT_ROOT).rglob(\"page_00.json\"))\n",
    "with open(some_file, encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"\\n\".join(itertools.islice((r[\"text\"] for r in data), 30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged text written to ../tmp/output/sample/sample.md\n"
     ]
    }
   ],
   "source": [
    "def merge_pages(pdf_stem: str):\n",
    "    page_dir = OUTPUT_ROOT / pdf_stem\n",
    "    md_path  = page_dir / f\"{pdf_stem}.md\"\n",
    "\n",
    "    page_files = sorted(page_dir.glob(\"page_*.json\"))\n",
    "    with open(md_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for p in page_files:\n",
    "            with open(p, encoding=\"utf-8\") as f:\n",
    "                objs = json.load(f)\n",
    "                lines = [o[\"text\"] for o in objs]\n",
    "                fout.write(f\"# {p.stem}\\n\\n\")\n",
    "                fout.write(\"\\n\".join(lines) + \"\\n\\n\")\n",
    "\n",
    "    return md_path\n",
    "\n",
    "merged = merge_pages(\"sample\")     # whatever your PDF stem is\n",
    "print(\"Merged text written to\", merged)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
