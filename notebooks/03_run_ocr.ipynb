{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# OCR Processing for PDF Page Images\n",
    "\n",
    "This notebook applies OCR to PNG page images (from PDFs in `../tmp/input_pdf`)  \n",
    "\n",
    "and saves the extracted text and metadata as JSON files in `../tmp/output/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# OCR Processing for PDF & Image Files\n",
    "# ====================================\n",
    "#\n",
    "# Renders PDF pages to PNG when needed, applies native-text/table extraction\n",
    "# or OCR/VLM for scanned pages, and writes one JSON per page (or per single image)\n",
    "# under ../tmp/output/.\n",
    "#\n",
    "# JSON schema per page:\n",
    "# {\n",
    "#   \"schema_version\": \"1.0\",\n",
    "#   \"page\": 0,\n",
    "#   \"size\": {\"width\": 1653, \"height\": 2338},\n",
    "#   \"items\": [\n",
    "#     {\"block_id\":\"p0_b0\",\"text\":\"…\",\"confidence\":0.95,\"bbox\":[l,t,r,b],\"page_index\":0}\n",
    "#   ]\n",
    "# }\n",
    "\n",
    "# 0. autoreload for rapid iteration\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Currently `results = ocr_engine.run_ocr(image)` in `process_document_folder(image_dir, output_dir)` is just a dummy\n",
    "# Ideas for models/approaches to try:\n",
    "# - SuryaOCR\n",
    "# - TrOCR\n",
    "# - Huggingface: SmolVLM, ColPali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. bootstrap paths & env\n",
    "import sys, os, shutil, importlib, json, itertools\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "notebook_dir   = Path.cwd().resolve()        # .../repo/notebooks\n",
    "repo_root      = notebook_dir.parent         # .../repo\n",
    "src_dir        = repo_root / \"src\"\n",
    "tmp_dir        = repo_root / \"tmp\"\n",
    "\n",
    "sys.path.extend([str(repo_root), str(src_dir)])\n",
    "load_dotenv(repo_root / \".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. check settings\n",
    "from config.settings import settings\n",
    "assert settings.model_type in (\"gemma\",\"smolvlm\",\"dummy\"), settings.model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tests.local_test' from '/Users/markuskuehnle/Documents/projects/credit-ocr-module/tests/local_test.py'>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. import & reload our unified OCR runner\n",
    "import tests.local_test as lt\n",
    "importlib.reload(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. prepare clean tmp/data and tmp/output\n",
    "data_dir   = tmp_dir / \"data\"\n",
    "output_dir = tmp_dir / \"output\"\n",
    "\n",
    "for d in (data_dir, output_dir):\n",
    "    if d.exists():\n",
    "        shutil.rmtree(d)\n",
    "    d.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. unify your input PDFs/images into tmp/input\n",
    "#    if you currently have a tmp/input_pdf folder, point here:\n",
    "input_pdf_dir = tmp_dir / \"input_pdf\"\n",
    "input_dir = tmp_dir / \"input\"\n",
    "\n",
    "if input_dir.exists():\n",
    "    shutil.rmtree(input_dir)\n",
    "input_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# copy all existing PDFs/images into tmp/input\n",
    "for ext in (\"*.pdf\",\"*.png\",\"*.jpg\",\"*.jpeg\"):\n",
    "    for f in (input_pdf_dir.glob(ext) if input_pdf_dir.exists() else []):\n",
    "        shutil.copy(f, input_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ processing sample.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:generic_ocr:00:28:45:Starting PDF sample.pdf with engine SmolVLMEngine\n",
      "INFO:generic_ocr:00:28:45:Page 00 (PDF) → page_00.json\n",
      "INFO:generic_ocr:00:28:45:Page 01 (PDF) → page_01.json\n",
      "INFO:generic_ocr:00:28:45:Page 02 (PDF) → page_02.json\n",
      "INFO:generic_ocr:00:28:46:Page 03 (PDF) → page_03.json\n",
      "INFO:generic_ocr:00:28:46:Page 04 (PDF) → page_04.json\n",
      "INFO:generic_ocr:00:28:47:Page 05 (PDF) → page_05.json\n",
      "INFO:generic_ocr:00:28:47:Page 06 (PDF) → page_06.json\n",
      "INFO:generic_ocr:00:28:47:Page 07 (PDF) → page_07.json\n",
      "INFO:generic_ocr:00:28:47:PDF sample.pdf done: 8 native, 0 OCR\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OCR done → JSON in /Users/markuskuehnle/Documents/projects/credit-ocr-module/tmp/output\n"
     ]
    }
   ],
   "source": [
    "# 6. run OCR/image processing on every file under tmp/input\n",
    "for file_path in sorted(input_dir.glob(\"*\")):\n",
    "    print(\"→ processing\", file_path.name)\n",
    "    lt.process_input(\n",
    "        file_path,\n",
    "        image_base_directory=data_dir,\n",
    "        json_output_root=output_dir,\n",
    "    )\n",
    "\n",
    "print(\"\\nOCR done → JSON in\", output_dir.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preview – document “sample”, page 0, size={'width': 612, 'height': 792}\n",
      "\n",
      "{\"text\": \"2020 33rd SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)\\nFrom explanations to feature selection: assessing\\nSHAP values as feature selection mechanism\\nWilson E. Marcilio-Jr and Danilo M. Eler\\nSa˜o Paulo State University - Department of Mathematics and Computer Science\\nPresidente Prudente, Sa˜o Paulo/Brazil\\nEmail: wilson.marcilio@unesp.br, danilo.eler@unesp.br\\nAbstract—Explainabilityhasbecomeoneofthemostdiscussed technique has the same problem of trying to choose a model\\ntopicsinmachinelearningresearchinrecentyears,andalthough for a task (e.g., in finance or clinical) only based on these\\nalotofmethodologiesthattrytoprovideexplanationstoblack-\\nmetrics; finally, although calculated as a part of the training\\nbox models have been proposed to address such an issue, little\\nprocess, embedded methods have to be incorporated based on\\ndiscussion has been made on the pre-processing steps involving\\nthe pipeline of development of machine learning solutions, such each model particularity, which could be difficult and tedious\\nas feature selection. In this work, we evaluate a game-theoretic to provide explanations for every different model.\\napproach used to explain the output of any machine learning In this work, we provide a methodology and assessment\\nmodel, SHAP, as a feature selection mechanism. In the experi-\\nfor feature selection based on model agnostic explanations,\\nments, we show that besides being able to explain the decisions\\nproduced by a novel approach know as SHAP [1]. The\\nof a model, it achieves better results than three commonly used\\nfeature selection algorithms. approach assigns SHAP values, which are contribution values\\nforamodel’soutput,foreachfeatureofeachdatapoint.These\\nI. INTRODUCTION\\nSHAP values encode the importance that a model gives for a\\nWorkingwithhigh-dimensionaldatasetshasbecomeacom- feature, so that, we use the contribution information of each\\nmon task for anyone working with data. While offering great feature to order the features based on its importance. In this\\nopportunitiestodiscoverpatternsandtendencies,dealingwith case, selecting a subset of d features based on SHAP values\\nhigh-dimensionaldatacanbecomplicatedduetotheso-called means to select the first d features after ordering them based\\ncurse of dimensionality. Essentially, the redundancy of the on the feature contributions to the model’s prediction. We\\ndataset increases as its dimensionality increases, which can validateourmethodologyonclassificationandregressiontasks\\nimpair the performance of various techniques. To overcome upon eight publicly available datasets. Experiments against\\nsuch issues, dimensionality reduction techniques, such as t- three common feature selection algorithms show that feature\\nSNE[3]orUMAP[4]canbeappliedtoreducedimensionality selection based on SHAP values presents the best results.\\nwhile maintaining as much of information as possible. One Summarily, the contributions of this work are:\\nproblem of such algorithms is that they remove the inter- • Assessment of SHAP as feature selection mechanism;\\npretability of the features (if they were interpretable at first) • A library with Python implementation of the methodol-\\nby applying series of non-linear equations and may introduce ogy1.\\nartifactsthatwerenotperceivedinthehigh-dimensionalspace.\\nThispaperisorganizedasfollows:inSectionIIwedescribe\\nOther approaches to deal with high dimensionality is to use\\ntherelatedworks;inSectionIIIweprovideabriefexplanation\\nfeatureselectionalgorithms,whichselectasubsetofvariables\\nof SHAP and delineate the methodology to perform feature\\nthat can describe the input data while proving good results in\\nselectionbasedonit;experimentsareprovidedinSectionIV;\\nprediction [5]. In this case, it is necessary to define a metric\\nwe discuss the results in Section V; the work is concluded in\\n(or selection criteria) in which the feature selection will be\\nSection VI.\\nbased[6],forexample,thecorrelationamongfeatures.Feature\\nselectionmethodsarecommonlydividedintothreecategories: II. RELATEDWORKS\\nfilter, wrapper, and embedded.\\nAs the dimensionality of datasets grows, the redundancy of\\nOne problem with traditional feature selection algorithms\\nthe data becomes a problem since with too many dimensions,\\nis related to their explainability issues. For example, when\\nevery data point in a dataset appears equidistant from the\\nworking with clinical data, how to explain that a few features\\nothers[7].Toreduceproblemslikethese,orsimplytofilterout\\nwere simply removed from the provided dataset? Each cate-\\nfeaturesthatarenotusefulforamachinelearningproblemand\\ngoryoffeatureselectionalgorithmhasitsweaknessonhowto\\ncan introduce artifacts to the dataset while demanding higher\\nexplain why certain features were chosen without diving into\\ntime execution, the number of features must be reduced.\\nthe mathematical formulation. That is, filter methods do not\\nFeatureselectionalgorithmsareusuallyclassifiedintothree\\nleverageamodel’scharacteristictofilterthefeatures;wrapper\\ngroups: filter, wrapper, and embedded [6], [8], [9]. Filter\\nmethodsdoleverageamodel’sprediction,however,tochoose\\na subset of feature only based on accuracy or another scoring 1https://github.com/wilsonjr/SHAP FSelection\\n2377-5416/20/$31.00 ©2020 IEEE 340\\nDOI 10.1109/SIBGRAPI51738.2020.00053\\nAuthorized licensed use limited to: Technische Hochschule Ulm. Downloaded on May 11,2025 at 13:08:26 UTC from IEEE Xplore. Restrictions apply.\", \"tables\": [[[\"\", \"Presidente Prudente, S˜ao Paulo/Brazil\"], [\"\", \"Email: wilson.marcilio@unesp.br, danilo.eler@unesp.br\"], [\"Abstract—Explainability has become one of the most discussed\", \"technique has the same problem of\\ntrying to choose a model\"], [\"topics in machine learning research in recent years, and although\", \"\"], [\"\", \"for\\na\\ntask (e.g.,\\nin ﬁnance or\\nclinical) only based on these\"], [\"a lot of methodologies that\\ntry to provide explanations to black-\", \"\"], [\"\", \"metrics; ﬁnally, although calculated as a part of\\nthe training\"], [\"box models have been proposed to address\\nsuch an issue,\\nlittle\", \"\"], [\"\", \"process, embedded methods have to be incorporated based on\"], [\"discussion has been made on the pre-processing steps\\ninvolving\", \"\"], [\"the pipeline of development of machine learning solutions, such\", \"each model particularity, which could be difﬁcult and tedious\"], [\"as\\nfeature selection.\\nIn this work, we evaluate a game-theoretic\", \"to provide explanations for every different model.\"], [\"approach used to explain the output of any machine\\nlearning\", \"\"], [\"\", \"In this work, we provide\\na methodology and assessment\"], [\"model, SHAP, as a feature selection mechanism.\\nIn the experi-\", \"\"], [\"\", \"for\\nfeature\\nselection based on model\\nagnostic\\nexplanations,\"], [\"ments, we show that besides being able to explain the decisions\", \"\"], [\"\", \"produced\\nby\\na\\nnovel\\napproach\\nknow as\\nSHAP\\n[1].\\nThe\"], [\"of a model,\\nit achieves better results than three commonly used\", \"\"], [\"feature selection algorithms.\", \"approach assigns SHAP values, which are contribution values\"], [\"\", \"for a model’s output, for each feature of each data point. These\"], [\"I.\\nINTRODUCTION\", \"\"], [\"\", \"SHAP values encode the importance that a model gives for a\"], [\"Working with high-dimensional datasets has become a com-\", \"feature,\\nso that, we use the contribution information of each\"], [\"mon task for anyone working with data. While offering great\", \"feature to order\\nthe features based on its\\nimportance.\\nIn this\"], [\"opportunities to discover patterns and tendencies, dealing with\", \"case, selecting a subset of d features based on SHAP values\"], [\"high-dimensional data can be complicated due to the so-called\", \"means to select\\nthe ﬁrst d features after ordering them based\"], [\"curse\\nof\\ndimensionality. Essentially,\\nthe\\nredundancy\\nof\\nthe\", \"on\\nthe\\nfeature\\ncontributions\\nto\\nthe model’s\\nprediction. We\"], [\"dataset\\nincreases\\nas\\nits dimensionality increases, which can\", \"validate our methodology on classiﬁcation and regression tasks\"], [\"impair\\nthe performance of various\\ntechniques. To overcome\", \"upon\\neight\\npublicly\\navailable\\ndatasets. Experiments\\nagainst\"], [\"such issues, dimensionality reduction techniques,\\nsuch as\\nt-\", \"three common feature selection algorithms\\nshow that\\nfeature\"], [\"SNE [3] or UMAP [4] can be applied to reduce dimensionality\", \"selection\\nbased\\non SHAP values\\npresents\\nthe\\nbest\\nresults.\"], [\"while maintaining as much of\\ninformation as possible. One\", \"Summarily,\\nthe contributions of\\nthis work are:\"], [\"problem of\\nsuch\\nalgorithms\\nis\\nthat\\nthey\\nremove\\nthe\\ninter-\", \"\"], [\"\", \"• Assessment of SHAP as feature selection mechanism;\"], [\"pretability of\\nthe features\\n(if\\nthey were interpretable at ﬁrst)\", \"\"], [\"\", \"• A library with Python implementation of\\nthe methodol-\"], [\"by applying series of non-linear equations and may introduce\", \"\"], [\"\", \"ogy1.\"], [\"artifacts that were not perceived in the high-dimensional space.\", \"\"], [\"\", \"This paper is organized as follows: in Section II we describe\"], [\"Other approaches\\nto deal with high dimensionality is\\nto use\", \"\"], [\"\", \"the related works; in Section III we provide a brief explanation\"], [\"feature selection algorithms, which select a subset of variables\", \"\"], [\"\", \"of SHAP and delineate the methodology to perform feature\"], [\"that can describe the input data while proving good results in\", \"\"], [\"\", \"selection based on it; experiments are provided in Section IV;\"], [\"prediction [5].\\nIn this case,\\nit\\nis necessary to deﬁne a metric\", \"\"], [\"\", \"we discuss the results in Section V;\\nthe work is concluded in\"], [\"(or\\nselection criteria)\\nin which the\\nfeature\\nselection will be\", \"\"], [\"\", \"Section VI.\"], [\"based [6], for example, the correlation among features. Feature\", \"\"], [\"selection methods are commonly divided into three categories:\", \"II. RELATED WORKS\"], [\"ﬁlter, wrapper, and embedded.\", \"\"], [\"\", \"As the dimensionality of datasets grows,\\nthe redundancy of\"], [\"One problem with traditional\\nfeature\\nselection algorithms\", \"\"], [\"\", \"the data becomes a problem since with too many dimensions,\"], [\"is\\nrelated to their\\nexplainability issues. For\\nexample, when\", \"\"], [\"\", \"every\\ndata\\npoint\\nin\\na\\ndataset\\nappears\\nequidistant\\nfrom the\"], [\"working with clinical data, how to explain that a few features\", \"\"], [\"\", \"others [7]. To reduce problems like these, or simply to ﬁlter out\"], [\"were simply removed from the provided dataset? Each cate-\", \"\"], [\"\", \"features that are not useful for a machine learning problem and\"], [\"gory of feature selection algorithm has its weakness on how to\", \"\"], [\"\", \"can introduce artifacts to the dataset while demanding higher\"], [\"explain why certain features were chosen without diving into\", \"\"], [\"\", \"time execution,\\nthe number of\\nfeatures must be reduced.\"], [\"the mathematical\\nformulation. That\\nis, ﬁlter methods do not\", \"\"], [\"\", \"Feature selection algorithms are usually classiﬁed into three\"], [\"leverage a model’s characteristic to ﬁlter the features; wrapper\", \"\"], [\"\", \"groups:\\nﬁlter, wrapper,\\nand\\nembedded\\n[6],\\n[8],\\n[9]. Filter\"], [\"methods do leverage a model’s prediction, however,\\nto choose\", \"\"], [\"a subset of feature only based on accuracy or another scoring\", \"1https://github.com/wilsonjr/SHAP FSelection\"]]]}\n"
     ]
    }
   ],
   "source": [
    "# 7. Preview first page of first document\n",
    "first_json = next(output_dir.rglob(\"page_00.json\"), None)\n",
    "if first_json is None:\n",
    "    raise RuntimeError(f\"No page_00.json found under {output_dir}\")\n",
    "\n",
    "page_data = json.loads(first_json.read_text())\n",
    "print(f\"\\nPreview – document “{first_json.parent.name}”, page {page_data['page']}, size={page_data['size']}\\n\")\n",
    "print(\"\\n\".join(itertools.islice((it[\"text\"] for it in page_data[\"items\"]), 30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged text written to /Users/markuskuehnle/Documents/projects/credit-ocr-module/tmp/output/sample/sample.md\n"
     ]
    }
   ],
   "source": [
    "# 8. Merge per-page JSON into one Markdown for quick inspection\n",
    "def merge_to_markdown(document_stem: str) -> Path:\n",
    "    \"\"\"\n",
    "    Concatenate all page_<n>.json under tmp/output/<document_stem>/ \n",
    "    into a single Markdown file for eyeballing.\n",
    "    \"\"\"\n",
    "    doc_dir = output_dir / document_stem\n",
    "    md_path = doc_dir / f\"{document_stem}.md\"\n",
    "    with open(md_path, \"w\", encoding=\"utf-8\") as md_file:\n",
    "        for page_json in sorted(doc_dir.glob(\"page_*.json\")):\n",
    "            data = json.loads(page_json.read_text())\n",
    "            md_file.write(f\"# page {data['page']}\\n\\n\")\n",
    "            for item in data[\"items\"]:\n",
    "                md_file.write(item[\"text\"] + \"\\n\\n\")\n",
    "    return md_path\n",
    "\n",
    "merged_md = merge_to_markdown(first_json.parent.name)\n",
    "print(\"Merged text written to\", merged_md.resolve())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
